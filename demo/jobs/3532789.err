Traceback (most recent call last):
  File "mongodemo.py", line 168, in <module>
    x, y = downsample_volume(x), downsample_volume(y)
  File "mongodemo.py", line 156, in downsample_volume
    downsampled = F.interpolate(volume.unsqueeze(0), size=new_size, mode='trilinear', align_corners=False)
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/site-packages/torch/nn/functional.py", line 3869, in interpolate
    raise ValueError(
ValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1, 256, 256, 256] and output size of [64, 64, 64]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 51, in _pin_memory_loop
    do_one_step()
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 28, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/data/users2/washbee/anaconda3/envs/neurips/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 307, in rebuild_storage_fd
